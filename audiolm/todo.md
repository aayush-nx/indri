D 1. create val set out if librispeech dev
D 2. tokens for all
D 3. generation + sampling + inference 
4. cleanup training script
W 5. setup wandb account
6. setup eternal training
7. next datasets -> 10k hours

3-10/6
1. continuous inference : listen 1/2 length, generate rest, repeat
2. sample of speech, continue speaking
3. 10B training run on jarvis, decide dataset