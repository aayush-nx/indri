D 1. create val set out if librispeech dev
D 2. tokens for all
D 3. generation + sampling + inference 
4. cleanup training script
W 5. setup wandb account
6. setup eternal training
7. next datasets -> 10k hours

3-10/6
0. extend tokenizer for large audio files
1. continuous inference : listen 1/2 length, generate rest, repeat, demo copy cat with continuation
2. sample of speech, continue speaking
3. 10B training run on jarvis, decide dataset
4. Finalize jarvis setup, notebooks OR fabfiles
5. Conversation data plan , movies and tornadoes
6. 1T audio data + training resource plan