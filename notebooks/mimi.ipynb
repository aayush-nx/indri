{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24945fc5-6ead-4fdc-b848-173763a3d6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apurva/miniconda3_11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/apurva/miniconda3_11/lib/python3.11/site-packages/transformers/models/mimi/modeling_mimi.py:217: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "from transformers import MimiModel, AutoFeatureExtractor\n",
    "librispeech_dummy = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "# load model and feature extractor\n",
    "model = MimiModel.from_pretrained(\"kyutai/mimi\")\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"kyutai/mimi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70f6baef-032b-4a9e-827f-0ea53daa384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d45f68c4-7151-4928-adb8-e4f2ed50a037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e50f2310-758a-4f2c-b184-01e1293e19aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files = glob.glob('/home/apurva/.cache/indri/shrutilipi/audio/*.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0911214-889d-4434-b84d-086588b60643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260806"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(audio_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e20d7e18-163d-46ae-bd20-dd189d995856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_audio(wav: torch.Tensor, sr: int, target_sr: int, target_channels: int):\n",
    "    assert wav.shape[0] in [1, 2], \"Audio must be mono or stereo.\"\n",
    "    if target_channels == 1:\n",
    "        wav = wav.mean(0, keepdim=True)\n",
    "    elif target_channels == 2:\n",
    "        *shape, _, length = wav.shape\n",
    "        wav = wav.expand(*shape, target_channels, length)\n",
    "    elif wav.shape[0] == 1:\n",
    "        wav = wav.expand(target_channels, -1)\n",
    "    wav = torchaudio.transforms.Resample(sr, target_sr)(wav)\n",
    "    return wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f13e97fd-1bfd-4aa4-8d60-b7d56111d0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260806/260806 [1:59:00<00:00, 36.52it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    for f in tqdm(audio_files):\n",
    "        x, sr = torchaudio.load(f)\n",
    "        x = convert_audio(x, sr=sr, target_sr=24000, target_channels=1)[0]\n",
    "        inputs = feature_extractor(raw_audio=x, sampling_rate=feature_extractor.sampling_rate, return_tensors=\"pt\").to('cuda:0')\n",
    "        encoder_outputs = model.encode(inputs[\"input_values\"], inputs[\"padding_mask\"], num_quantizers=8)\n",
    "        out_f = f.replace('/audio/', '/tokens/mimi/').replace('.wav', '.npy')\n",
    "        audio_codes = encoder_outputs.audio_codes.cpu().numpy()[0]\n",
    "        np.save(out_f, audio_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5292348f-bf80-4ad4-bddb-57f66529be68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
