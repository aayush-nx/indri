{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c982fabb-4453-4518-aabc-08f5f4aa473e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache directory at:  /home/apurva/.cache/indri\n",
      "Gap tokens:  1519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apurva/miniconda3_11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torch.cuda import empty_cache\n",
    "from IPython.display import Audio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from commons import DEVICE, CACHE_DIR, CTX,  TEXT, MIMI, SPEAKER_FILE, CONVERT\n",
    "from commons import Config as cfg\n",
    "from omni.hfload import convert_to_hf\n",
    "from omni.tokenlib import get_tokenizer\n",
    "from omni.gpt2_model import GPT, GPTConfig\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c656848-921e-4b29-9635-29456ac0d28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-03 08:11:11.%f | INFO     | train_with_mimi.py:16 | {'__module__': 'commons', 'coarse_codebooks': 2, 'per_codebook_size': 1024, 'VOCAB_SIZES': {'text': 50257, 'mimi': 8192}, 'OFFSET': {'text': 0, 'mimi': 50257}, 'TASK_TOKENS': {'convert': '[convert]', 'continue': '[continue]'}, 'MODALITY_TOKENS': {'text': '[text]', 'mimi': '[mimi]'}, 'UNKNOWN_SPEAKER_ID': '[spkr_unk]', 'STOP_TOKEN': '[stop]', 'VOCAB_SIZE': 59968, '__dict__': <attribute '__dict__' of 'Config' objects>, '__weakref__': <attribute '__weakref__' of 'Config' objects>, '__doc__': None}\n"
     ]
    }
   ],
   "source": [
    "from train_with_mimi import get_text_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20b9ff0f-2b21-49e6-a31a-afdf2f6ebe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe4f1fa8-3ea6-4b2c-b3a4-ea657d1d3a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apurva/projects/indri/omni/../omni/hfload.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  custom_gpt = torch.load(path, map_location=device)['model']\n",
      "/home/apurva/projects/indri/omni/../omni/hfload.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  custom_gpt_config = torch.load(path, map_location=device)['config']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded config GPTConfig(block_size=1024, vocab_size=59968, n_layer=36, n_head=20, n_embd=1280, dropout=0.0, bias=True)\n"
     ]
    }
   ],
   "source": [
    "omni_model = convert_to_hf('/home/apurva/Downloads/gpt_136000.pt', device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b97da21-4b11-4986-92b5-0963e5fca14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apurva/miniconda3_11/lib/python3.11/site-packages/transformers/models/mimi/modeling_mimi.py:217: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "from transformers import MimiModel, AutoFeatureExtractor\n",
    "librispeech_dummy = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "# load model and feature extractor\n",
    "model = MimiModel.from_pretrained(\"kyutai/mimi\")\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"kyutai/mimi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cce9a2d-f964-4ec6-a292-4bb4b9c5bbd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a98cc5a0-5dc0-4081-bd37-da65441d8ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text vocab size 50257\n"
     ]
    }
   ],
   "source": [
    "text_tokenizer = get_text_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b287b0c-4d8f-4742-8958-da13b507b3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_token = text_tokenizer.encode(cfg.TASK_TOKENS[CONVERT])\n",
    "stop_token = text_tokenizer.encode(cfg.STOP_TOKEN)\n",
    "\n",
    "text_modality_token = text_tokenizer.encode(cfg.MODALITY_TOKENS[TEXT])\n",
    "acoustic_modality_token = text_tokenizer.encode(cfg.MODALITY_TOKENS[MIMI])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "94857a1c-7f9d-4946-aafd-6da5e8c2f4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'hey we are indri labs and we are glad to present you the super fast text to speech system that we have built'\n",
    "txt_toks = text_tokenizer.encode(text)\n",
    "speaker_id = text_tokenizer.encode(cfg.UNKNOWN_SPEAKER_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5c10a0be-7be7-4232-b5a2-dffcfb3def07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text tokens: torch.Size([1, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[text]hey we are indri labs and we are glad to present you the super fast text to speech system that we have built[convert][mimi][spkr_unk]'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens = np.hstack([\n",
    "    text_modality_token,\n",
    "    txt_toks,\n",
    "    convert_token,\n",
    "    acoustic_modality_token,\n",
    "    speaker_id,\n",
    "])\n",
    "input_tokens = (torch.tensor(input_tokens, dtype=torch.long, device=DEVICE)[None, ...])\n",
    "print(f'Text tokens: {input_tokens.shape}')\n",
    "text_tokenizer.decode(input_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bdf977b4-a259-4e42-84a5-df355fa8bad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LogitsProcessor\n",
    "class AlternatingCodebooksLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, input_start_len: int, codebook_size: int, num_codebooks: int, offset: int, stop_token: int):\n",
    "        self.input_start_len = input_start_len\n",
    "        self.codebook_size = codebook_size\n",
    "        self.num_codebooks = num_codebooks\n",
    "        self.offset = offset\n",
    "        self.stop_token = stop_token\n",
    "    \n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        curr_len = input_ids.shape[-1]\n",
    "        codebook_idx = ((curr_len - self.input_start_len) % self.num_codebooks)\n",
    "        \n",
    "        scores_processed = scores.clone()\n",
    "        scores_processed[:, : self.offset + codebook_idx * self.codebook_size] = -float(\"inf\")\n",
    "        scores_processed[:, self.offset + (codebook_idx+1) * self.codebook_size :] = -float(\"inf\")\n",
    "        scores_processed[:, stop_token] = scores[:, stop_token]\n",
    "        return scores_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "86f72048-9d15-4e6a-a9c6-3ce3091e0d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 365)\n"
     ]
    }
   ],
   "source": [
    "with CTX:\n",
    "    omni_model.generation_config.eos_token_id = stop_token\n",
    "    semantic_tokens = omni_model.generate(\n",
    "        input_tokens,\n",
    "        max_length=1024,\n",
    "        temperature=0.6,\n",
    "        top_k=30,\n",
    "        do_sample=True,\n",
    "        logits_processor=[AlternatingCodebooksLogitsProcessor(input_start_len=len(input_tokens[0]),\n",
    "                                                              codebook_size=2048,\n",
    "                                                              num_codebooks=4,\n",
    "                                                              offset=cfg.OFFSET[MIMI],\n",
    "                                                             stop_token=stop_token)]\n",
    "    )\n",
    "    semantic_tokens = semantic_tokens.detach().cpu().numpy()\n",
    "    print(semantic_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "98825ab6-54b1-42ed-9ee3-a2f2cd4a4296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[aco_1049][aco_2848][aco_5690][aco_7706][aco_1946][aco_2291][aco_5838][aco_7492][aco_1156][aco_3567][aco_4879][aco_7856][aco_420][aco_2519][aco_4456][aco_7236][aco_225][aco_3590][aco_5977][aco_6418][aco_915][aco_3052][aco_5024][aco_6902][aco_1828][aco_2323][aco_5576][aco_6985][aco_281][aco_2323][aco_5477][aco_6748][aco_285][aco_3945][aco_5477][aco_6704][aco_1546][aco_3831][aco_4977][aco_7762][aco_220][aco_2833][aco_5636][aco_6867][aco_1989][aco_2068][aco_5024][aco_6426][aco_929][aco_2349][aco_4740][aco_6416][aco_929][aco_2349][aco_4403][aco_7378][aco_1014][aco_2861][aco_4898][aco_6545][aco_1781][aco_3317][aco_4602][aco_6321][aco_1536][aco_3431][aco_4560][aco_6418][aco_342][aco_2481][aco_4626][aco_6969][aco_1592][aco_3900][aco_5792][aco_6854][aco_371][aco_3138][aco_4200][aco_8058][aco_75][aco_3442][aco_5083][aco_7942][aco_90][aco_2099][aco_6080][aco_7888][aco_1222][aco_2099][aco_5060][aco_6330][aco_1222][aco_2099][aco_5216][aco_7964][aco_1822][aco_3090][aco_4305][aco_7727][aco_1916][aco_3113][aco_4807][aco_6857][aco_1602][aco_3203][aco_5595][aco_6683][aco_1931][aco_2387][aco_4722][aco_6835][aco_845][aco_3343][aco_4733][aco_7680][aco_257][aco_3431][aco_5366][aco_7479][aco_1368][aco_3817][aco_5712][aco_8077][aco_1657][aco_2613][aco_5024][aco_6869][aco_1253][aco_2952][aco_4763][aco_7620][aco_1344][aco_4029][aco_4301][aco_6282][aco_90][aco_3784][aco_5959][aco_7176][aco_237][aco_2666][aco_4543][aco_7622][aco_2030][aco_2834][aco_4257][aco_6969][aco_284][aco_3360][aco_6063][aco_6327][aco_313][aco_3930][aco_5033][aco_8079][aco_1997][aco_3022][aco_4391][aco_6913][aco_199][aco_3203][aco_4508][aco_7300][aco_1109][aco_2448][aco_4634][aco_7578][aco_1109][aco_3948][aco_4543][aco_6447][aco_786][aco_2449][aco_5564][aco_6348][aco_786][aco_3948][aco_4278][aco_7622][aco_272][aco_3483][aco_5419][aco_6737][aco_1967][aco_3203][aco_4096][aco_7280][aco_1112][aco_3824][aco_4172][aco_7303][aco_887][aco_3216][aco_4558][aco_7856][aco_1295][aco_3492][aco_5947][aco_7407][aco_1767][aco_2806][aco_5705][aco_7217][aco_1356][aco_2414][aco_4265][aco_7594][aco_101][aco_2099][aco_5473][aco_6443][aco_2027][aco_2242][aco_5572][aco_7630][aco_1615][aco_3891][aco_5243][aco_6966][aco_912][aco_2079][aco_5706][aco_7354][aco_847][aco_2779][aco_4733][aco_7616][aco_1580][aco_3360][aco_5308][aco_6865][aco_984][aco_3544][aco_6079][aco_6592][aco_1658][aco_3792][aco_5498][aco_6835][aco_912][aco_3824][aco_5670][aco_7571][aco_1967][aco_3956][aco_5869][aco_8086][aco_1762][aco_3207][aco_5015][aco_8086][aco_96][aco_3632][aco_4135][aco_7533][aco_1333][aco_2123][aco_4264][aco_6327][aco_327][aco_3203][aco_5627][aco_6436][aco_735][aco_2826][aco_5134][aco_6174][aco_1985][aco_3288][aco_5298][aco_7620][aco_1615][aco_2387][aco_4762][aco_6835][aco_1304][aco_2778][aco_5904][aco_7837][aco_1304][aco_2081][aco_5906][aco_7676][aco_1747][aco_3411][aco_4454][aco_6258][aco_1612][aco_2984][aco_4222][aco_7354][aco_1250][aco_2214][aco_4566][aco_7826][aco_345][aco_3632][aco_4487][aco_6827][aco_1085][aco_3360][aco_4553][aco_7715][aco_1865][aco_3104][aco_4879][aco_7492][aco_1324][aco_4090][aco_5366][aco_7525][aco_668][aco_4035][aco_5863][aco_6873][aco_1039][aco_2123][aco_5722][aco_7061][aco_579][aco_3141][aco_4419][aco_7478][aco_1448][aco_3567][aco_5793][aco_6308][aco_1814][aco_3668][aco_5020][aco_8061][aco_1771][aco_3711][aco_4879][aco_7706][stop]'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sem_tokens = semantic_tokens[0][len(input_tokens[0]):]\n",
    "text_tokenizer.decode(sem_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "84a0add2-3524-401b-b516-86e20460bcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last = None\n",
    "last = np.where(sem_tokens==stop_token)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "81d17fe9-b60b-47ce-8c79-629fd89e4f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1049, 2848, 5690, 7706, 1946, 2291, 5838, 7492, 1156, 3567, 4879,\n",
       "       7856,  420, 2519, 4456, 7236,  225, 3590, 5977, 6418,  915, 3052,\n",
       "       5024, 6902, 1828, 2323, 5576, 6985,  281, 2323, 5477, 6748,  285,\n",
       "       3945, 5477, 6704, 1546, 3831, 4977, 7762,  220, 2833, 5636, 6867,\n",
       "       1989, 2068, 5024, 6426,  929, 2349, 4740, 6416,  929, 2349, 4403,\n",
       "       7378, 1014, 2861, 4898, 6545, 1781, 3317, 4602, 6321, 1536, 3431,\n",
       "       4560, 6418,  342, 2481, 4626, 6969, 1592, 3900, 5792, 6854,  371,\n",
       "       3138, 4200, 8058,   75, 3442, 5083, 7942,   90, 2099, 6080, 7888,\n",
       "       1222, 2099, 5060, 6330, 1222, 2099, 5216, 7964, 1822, 3090, 4305,\n",
       "       7727, 1916, 3113, 4807, 6857, 1602, 3203, 5595, 6683, 1931, 2387,\n",
       "       4722, 6835,  845, 3343, 4733, 7680,  257, 3431, 5366, 7479, 1368,\n",
       "       3817, 5712, 8077, 1657, 2613, 5024, 6869, 1253, 2952, 4763, 7620,\n",
       "       1344, 4029, 4301, 6282,   90, 3784, 5959, 7176,  237, 2666, 4543,\n",
       "       7622, 2030, 2834, 4257, 6969,  284, 3360, 6063, 6327,  313, 3930,\n",
       "       5033, 8079, 1997, 3022, 4391, 6913,  199, 3203, 4508, 7300, 1109,\n",
       "       2448, 4634, 7578, 1109, 3948, 4543, 6447,  786, 2449, 5564, 6348,\n",
       "        786, 3948, 4278, 7622,  272, 3483, 5419, 6737, 1967, 3203, 4096,\n",
       "       7280, 1112, 3824, 4172, 7303,  887, 3216, 4558, 7856, 1295, 3492,\n",
       "       5947, 7407, 1767, 2806, 5705, 7217, 1356, 2414, 4265, 7594,  101,\n",
       "       2099, 5473, 6443, 2027, 2242, 5572, 7630, 1615, 3891, 5243, 6966,\n",
       "        912, 2079, 5706, 7354,  847, 2779, 4733, 7616, 1580, 3360, 5308,\n",
       "       6865,  984, 3544, 6079, 6592, 1658, 3792, 5498, 6835,  912, 3824,\n",
       "       5670, 7571, 1967, 3956, 5869, 8086, 1762, 3207, 5015, 8086,   96,\n",
       "       3632, 4135, 7533, 1333, 2123, 4264, 6327,  327, 3203, 5627, 6436,\n",
       "        735, 2826, 5134, 6174, 1985, 3288, 5298, 7620, 1615, 2387, 4762,\n",
       "       6835, 1304, 2778, 5904, 7837, 1304, 2081, 5906, 7676, 1747, 3411,\n",
       "       4454, 6258, 1612, 2984, 4222, 7354, 1250, 2214, 4566, 7826,  345,\n",
       "       3632, 4487, 6827, 1085, 3360, 4553, 7715, 1865, 3104, 4879, 7492,\n",
       "       1324, 4090, 5366, 7525,  668, 4035, 5863, 6873, 1039, 2123, 5722,\n",
       "       7061,  579, 3141, 4419, 7478, 1448, 3567, 5793, 6308, 1814, 3668,\n",
       "       5020, 8061, 1771, 3711, 4879, 7706])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_tokens = sem_tokens[:last] - cfg.OFFSET[MIMI]\n",
    "audio_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7adc4018-7655-4ffd-a38c-fd09aa138437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deserialize_tokens(tokens):\n",
    "    cb1 = tokens[::4]\n",
    "    cb2 = tokens[1::4]\n",
    "    cb3 = tokens[2::4]\n",
    "    cb4 = tokens[3::4]\n",
    "    min_shape = min(cb1.shape, cb2.shape, cb3.shape, cb4.shape)[0]\n",
    "    acoustic_tokens = np.stack([cb1[:min_shape], cb2[:min_shape] - 2048, cb3[:min_shape] - 4096, cb4[:min_shape] - 6144])\n",
    "    return acoustic_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0c792dca-a1c2-4ae2-8342-1c9e41b4e4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mimi_tokens = deserialize_tokens(audio_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948948a2-47c2-46b9-9ad3-4fdb12796d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a313df05-9a07-4598-8d2a-9ff2ec039da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1049, 1946, 1156,  420,  225,  915, 1828,  281,  285, 1546,  220,\n",
       "        1989,  929,  929, 1014, 1781, 1536,  342, 1592,  371,   75,   90,\n",
       "        1222, 1222, 1822, 1916, 1602, 1931,  845,  257, 1368, 1657, 1253,\n",
       "        1344,   90,  237, 2030,  284,  313, 1997,  199, 1109, 1109,  786,\n",
       "         786,  272, 1967, 1112,  887, 1295, 1767, 1356,  101, 2027, 1615,\n",
       "         912,  847, 1580,  984, 1658,  912, 1967, 1762,   96, 1333,  327,\n",
       "         735, 1985, 1615, 1304, 1304, 1747, 1612, 1250,  345, 1085, 1865,\n",
       "        1324,  668, 1039,  579, 1448, 1814, 1771],\n",
       "       [ 800,  243, 1519,  471, 1542, 1004,  275,  275, 1897, 1783,  785,\n",
       "          20,  301,  301,  813, 1269, 1383,  433, 1852, 1090, 1394,   51,\n",
       "          51,   51, 1042, 1065, 1155,  339, 1295, 1383, 1769,  565,  904,\n",
       "        1981, 1736,  618,  786, 1312, 1882,  974, 1155,  400, 1900,  401,\n",
       "        1900, 1435, 1155, 1776, 1168, 1444,  758,  366,   51,  194, 1843,\n",
       "          31,  731, 1312, 1496, 1744, 1776, 1908, 1159, 1584,   75, 1155,\n",
       "         778, 1240,  339,  730,   33, 1363,  936,  166, 1584, 1312, 1056,\n",
       "        2042, 1987,   75, 1093, 1519, 1620, 1663],\n",
       "       [1594, 1742,  783,  360, 1881,  928, 1480, 1381, 1381,  881, 1540,\n",
       "         928,  644,  307,  802,  506,  464,  530, 1696,  104,  987, 1984,\n",
       "         964, 1120,  209,  711, 1499,  626,  637, 1270, 1616,  928,  667,\n",
       "         205, 1863,  447,  161, 1967,  937,  295,  412,  538,  447, 1468,\n",
       "         182, 1323,    0,   76,  462, 1851, 1609,  169, 1377, 1476, 1147,\n",
       "        1610,  637, 1212, 1983, 1402, 1574, 1773,  919,   39,  168, 1531,\n",
       "        1038, 1202,  666, 1808, 1810,  358,  126,  470,  391,  457,  783,\n",
       "        1270, 1767, 1626,  323, 1697,  924,  783],\n",
       "       [1562, 1348, 1712, 1092,  274,  758,  841,  604,  560, 1618,  723,\n",
       "         282,  272, 1234,  401,  177,  274,  825,  710, 1914, 1798, 1744,\n",
       "         186, 1820, 1583,  713,  539,  691, 1536, 1335, 1933,  725, 1476,\n",
       "         138, 1032, 1478,  825,  183, 1935,  769, 1156, 1434,  303,  204,\n",
       "        1478,  593, 1136, 1159, 1712, 1263, 1073, 1450,  299, 1486,  822,\n",
       "        1210, 1472,  721,  448,  691, 1427, 1942, 1942, 1389,  183,  292,\n",
       "          30, 1476,  691, 1693, 1532,  114, 1210, 1682,  683, 1571, 1348,\n",
       "        1381,  729,  917, 1334,  164, 1917, 1562]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimi_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5d0bf041-fbba-4d79-b55f-ade3e4896165",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.decode(torch.tensor(np.expand_dims(mimi_tokens, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "231df786-c8ec-4305-a5ee-5d9ec780abed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 161280])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.audio_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "96085659-00ef-4a5d-9895-e22255128429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "53cc3e1e-7853-462c-82ff-8618662a9e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchaudio.save('test.wav', out.audio_values[0],sample_rate=24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5356643-572e-45b9-890f-c512cc33df30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f6df0e-c2f6-40cd-a07e-0fd1daf825dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70437ca8-5f99-4067-a3ea-caa63f24793c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994ac3dd-8c55-45dd-a063-a3afa6f7cc80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f063818d-e0df-443e-a507-c08ad412efe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d78f7e-d7f0-4aeb-976d-e7f2ab39ea6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a304602c-a20d-45a0-935b-e4d3f2a416d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b604a694-6186-44a9-9f29-b3e932b6919c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4c2ba9-d471-410b-9a0e-9e25ae417a61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd3c125-a75e-4088-b5ed-ba94fc03bd8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
